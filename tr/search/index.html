<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!-- JQuery (used for bootstrap and jekyll search) -->
    <script src="/assets/js/jquery-3.2.1.min.js" ></script>

    <!-- Main JS (navbar.js and katex_init.js)-->
    <script defer=true src="/assets/js/main.min.js"></script>

    <!-- CSS -->
    <link rel="stylesheet" href="/tr/assets/css/main.css">

    <!--Favicon-->
    <link rel="shortcut icon" href="/tr/assets/favicon.ico" type="image/x-icon">

    <!-- Canonical -->
    <link rel="canonical" href="https://blgnksy.github.io/tr/search/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="NDeep" href="https://blgnksy.github.io/tr///feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="/tr//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/tr/assets/css/font-awesome.min.css">

    <!-- Google Fonts -->
    
    <link href="/tr//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
    

    <!-- KaTeX 0.8.3 -->
    
    <!--<link rel="stylesheet" href="/tr//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.js"></script> -->
    <link rel="stylesheet" type="text/css" href="/tr/assets/css/katex.min.css">
    <script src="/assets/js/katex.min.js">
    </script>
    


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NWPYEC2Z49"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NWPYEC2Z49');
    </script>


    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'G-NWPYEC2Z49', 'auto');
        ga('send', 'pageview');

    </script>
    

    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Search | Blog About Deep/Machine Learning, CUDA, Computer Vision</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Search" />
<meta property="og:locale" content="tr" />
<meta name="description" content="A website with blog posts and pages on mostly on Docker, Computer Vision, Machine Learning, Deep Learning and same kind of software related topics. Çoğunlukla Docker, Bilgisayarlı Görü, Makine Öğrenmesi,Derin Öğrenme ve benzeri yazılımla ilgili konularda bloglar yayımlanan bir websitesi." />
<meta property="og:description" content="A website with blog posts and pages on mostly on Docker, Computer Vision, Machine Learning, Deep Learning and same kind of software related topics. Çoğunlukla Docker, Bilgisayarlı Görü, Makine Öğrenmesi,Derin Öğrenme ve benzeri yazılımla ilgili konularda bloglar yayımlanan bir websitesi." />
<link rel="canonical" href="https://blgnksy.github.io/tr/search/" />
<meta property="og:url" content="https://blgnksy.github.io/search/" />
<meta property="og:site_name" content="Blog About Deep/Machine Learning, CUDA, Computer Vision" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Search" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A website with blog posts and pages on mostly on Docker, Computer Vision, Machine Learning, Deep Learning and same kind of software related topics. Çoğunlukla Docker, Bilgisayarlı Görü, Makine Öğrenmesi,Derin Öğrenme ve benzeri yazılımla ilgili konularda bloglar yayımlanan bir websitesi.","headline":"Search","url":"https://blgnksy.github.io/search/"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Manual seo tags -->
    <!--
    <title>Search | NDeep</title>
    <meta name="description" content="">
    -->

    <!-- Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "WebSite",
      "name": "NDeep",
      "url": "https://blgnksy.github.io",
      "description": "",
      "inLanguage": "tr",
      "potentialAction": {
        "@type": "SearchAction",
        "target": "https://blgnksy.github.io/search/?q={search_term_string}",
        "query-input": "required name=search_term_string"
      }
    }
    </script>
</head>

  <body>
    <header class="site-header">
    
    <!-- Logo and title -->
	<div class="branding">
		<a href="/tr/">
			<img class="avatar" src="/assets/img/triangle.svg" alt=""/>
		</a>

		<h1 class="site-title">
			<a href="/tr/">NDeep</a>
		</h1>
	</div>
    
    <!-- Toggle menu -->
    <nav class="clear">
    <a id="pull" class="toggle" href="#">
    <i class="fa fa-bars fa-lg"></i>
    </a>
    
    <!-- Menu -->
    <ul>
        
        
        
        
        <li class="separator">
            |
        </li>
        <li>
            <a class="clear" href="/tr/about/">
                Hakkımda
            </a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
         
        
        
        <li class="separator">
            |
        </li>
        <li>
            <a class="clear" href="https://blgnksy.github.io/tr/search">
                <i class="fa fa-search" aria-hidden="true"></i>
            </a>
        </li>
        
        
        <li class="separator">
            |
        </li>
        <li>
            <a class="clear" href="https://blgnksy.github.io/tr/tags">
                <i class="fa fa-tags" aria-hidden="true"></i>
            </a>
        </li>
        
        
        
        
    </ul>
        
	</nav>
</header>

    <div class="content">
      <article class="feature-image">
  <header id="main" style="background-image: url('/assets/img/pexels/search-map.jpeg')">
    <h1 id="Search" class="title">
        Search
    </h1>
    
    
    <h2 class="subtitle">What are you looking for?</h2>
    
      
  </header>
  <section class="post-content"><!-- Html Elements for Search -->
<input type="text" id="search-input" placeholder="Enter keywords..." class="search-bar">
<br>
<br>
<ul id="results-container"></ul>

<section>
    <!-- Script pointing to jekyll-search.js -->
    <script src="/assets/js/simple-jekyll-search.min.js" type="text/javascript"></script>

    <script type="text/javascript">
        SimpleJekyllSearch({
            searchInput: document.getElementById('search-input'),
            resultsContainer: document.getElementById('results-container'),
            json: [
                    
                     
                        {
                          "title"    : "Minimalist OS",
                          "category" : "",
                          "tags"     : " Linux, Busybox, Boot Sequence",
                          "url"      : "/2025/12/22/minimal-os.html",
                          "date"     : "December 22, 2025",
                          "excerpt"  : "I have been using Linux as my daily driver—both professionally and personally—for many years. My fascination with kernel internals has led me to build the kernel myself and boot it inside a virtual machine (I use QEMU). I’m planning another blog p...",
                          "content"  : "I have been using Linux as my daily driver—both professionally and personally—for many years. My fascination with kernel internals has led me to build the kernel myself and boot it inside a virtual machine (I use QEMU). I’m planning another blog post about kernel compilation, but for now I’ll focus on how to create a bootable ISO image for Linux.The example use‑cases of the need to create an ISO image will be defined later, but first let’s review the general boot process:      Firmware initialization (BIOS/UEFI)When the system powers on, the firmware runs POST, initializes hardware, and looks for a bootable device.Legacy BIOS loads the first‑stage bootloader into RAM; modern systems with UEFI load an EFI application (e.g., bootx64.efi).        Bootloader (GRUB, Windows Boot Manager, etc.)The bootloader switches the CPU to the appropriate execution mode, loads the Linux kernel image (bzImage) and the initial RAM filesystem (initramfs, usually initrd.img), then transfers control to the kernel entry point.        Kernel decompression and early initializationThe kernel decompresses itself to its final location, jumps to the architecture‑specific entry point, and calls start_kernel(). This routine sets up memory management, the scheduler, interrupt handling, core subsystems, and mounts the temporary root filesystem (the initramfs) in preparation for userspace.        Early userspace (initramfs)The kernel runs the first userspace program, trying the following in order:          /init      /sbin/init      /etc/init      /bin/init      /bin/sh        During this phase the initramfs populates /dev (normally via udev), loads any required kernel modules, and mounts the virtual filesystems /proc and /sys.        Root‑filesystem handoffOnce the real root filesystem is ready, the kernel switches to it using pivot_root or switch_root, then re‑executes the init process from the new root. This marks the transition from early userspace to the main userspace environment.        Init system startupPID 1 (usually systemd, but it could be OpenRC, SysVinit, etc.) starts system services and configures console devices.        Shell executionAfter the init system finishes, a login shell is presented. The system is now fully booted, and any userspace program can be launched.  As I mentioned before, there would be some other use cases where a user needs to create an ISO image.Use cases  Creating your own distro,  Creating system recovery and rescue environments,  Implementing/testing/debugging new kernel features      Educational and research needs  Preparing the ISOFirst, locate a kernel image and its initramfs. In a shell, list the contents of /boot:ls -la /bootTypical output (Fedora 43 example):total 625628dr-xr-xr-x. 6 root root      4096 Dec 21 09:31 .dr-xr-xr-x. 1 root root       192 Dec 22 14:32 ..-rw-r--r--. 1 root root    292973 Dec 13 01:00 config-6.17.12-300.fc43.x86_64-rw-r--r--. 1 root root    292938 Oct  6 02:00 config-6.17.1-300.fc43.x86_64drwx------. 4 root root      4096 Jan  1  1970 efidrwx------. 3 root root      4096 Dec 22 15:36 grub2-rw-------. 1 root root 270890527 Dec 20 15:51 initramfs-0-rescue-70a9f446bdd94bfb904e2762ff2f9046.img-rw-------. 1 root root 146082930 Dec 21 09:31 initramfs-6.17.12-300.fc43.x86_64.img-rw-------. 1 root root 146076670 Dec 21 09:30 initramfs-6.17.1-300.fc43.x86_64.imgdrwxr-xr-x. 3 root root      4096 Dec 20 15:49 loaderdrwx------. 2 root root     16384 Dec 20 15:46 lost+foundlrwxrwxrwx. 1 root root        47 Dec 20 16:17 symvers-6.17.12-300.fc43.x86_64.xz -&gt; /lib/modules/6.17.12-300.fc43.x86_64/symvers.xzlrwxrwxrwx. 1 root root        46 Dec 20 15:49 symvers-6.17.1-300.fc43.x86_64.xz -&gt; /lib/modules/6.17.1-300.fc43.x86_64/symvers.xz-rw-r--r--. 1 root root  11127277 Dec 13 01:00 System.map-6.17.12-300.fc43.x86_64-rw-r--r--. 1 root root  12017392 Oct  6 02:00 System.map-6.17.1-300.fc43.x86_64-rwxr-xr-x. 1 root root  18184232 Dec 13 01:00 vmlinuz-6.17.12-300.fc43.x86_64-rwxr-xr-x. 1 root root  17807720 Oct  6 02:00 vmlinuz-6.17.1-300.fc43.x86_64The file you’ll need now is the kernel image (vmlinuz‑). Create a working directory and copy the kernel:mkdir custom_iso &amp;&amp; cd custom_isocp /boot/vmlinuz-6.17.12-300.fc43.x86_64 bzImage   # rename as you likeAdding a Minimal BusyBox UserspaceIf you’re not familiar with BusyBox, see its documentation: https://busybox.net/about.html. We’ll use it as a tiny, static userspace.# Download and extract BusyBoxwget https://busybox.net/downloads/busybox-1.37.0.tar.bz2tar xf busybox-1.37.0.tar.bz2cd busybox-1.37.0# Default config → enable static linkingmake defconfigsed -i 's/# CONFIG_STATIC is not set/CONFIG_STATIC=y/' .config# Build a statically linked binaryLDFLAGS=\"--static\" make -j$(nproc) busyboxcd ..Building the InitramfsCreate the minimal root filesystem hierarchy:mkdir -p initrd/{bin,dev,proc,sys,root}Populate /bin with BusyBox and symlinks for each provided utility:cd initrd/bincp ../../busybox-1.37.0/busybox .chmod +x busybox# Create a symlink for every BusyBox appletfor util in $(./busybox --list); do    ln -s ./busybox \"$util\"donecd ..We only need to craft our initialization script. A detailed list of responsibilities of a PID 1 are:  Starts/monitors essential system processes,  Handles the signals such as SIGTERM, SIGINT, SIGCHLD,  Mounts virtual filesystems (/proc, /sys, /dev),  Loads kernel modules,  Ensures /dev/console exists,  Sets up stdin/stdout/stderr,  Starts login or shell on consoles,  Establishes base environment variables,  Mounts the real root filesystem,  Performs switch_root or pivot_root,  Fees initramfs memory,  Handles poweroff, reboot, halt requests, Ctrl-Alt-Del behavior as well as SIGPWR, and SIGWINCH,  Invokes kernel reboot/poweroff.We want out initialization script (our PID 1) to mount virtual filesystem, to provide device nodes, to set some kernel logging configuration up, and to run a shell.Write a very small init script (PID 1) that mounts the virtual filesystems and drops to a shell:cat &gt; init &lt;&lt;'EOF'#!/bin/shmount -t sysfs sysfs /sysmount -t proc proc /procmount -t devtmpfs udev /devsysctl -w kernel.printk=\"2 4 1 7\"clearexec /bin/shEOFchmod +x initPackage the initramfs using cpio (newc format is required by the Linux kernel):chmod -R 777 .find . | cpio -o -H newc &gt; ../initrd.imgcd ..Testing the kernel and initial ram disk with qemuqemu-system-x86_64 -m 4096 -smp 6  -kernel bzImage -initrd initrd.img You should be able to see the command prompt:Actually this is enough to test/debug the kernel. You don’t need an ISO.Creating the Bootable ISO with GRUB# Directory layout expected by grub-mkrescuemkdir -p iso/boot/grub# Copy kernel and initramfscp bzImage iso/boot/cp initrd.img iso/boot/# GRUB configurationcat &gt; iso/boot/grub/grub.cfg &lt;&lt;'EOF'set timeout=5set default=0menuentry \"Custom Linux with BusyBox userspace\" {    linux /boot/bzImage    initrd /boot/initrd.img}EOF# Build the ISOgrub2-mkrescue -o custom_os.iso iso/ # On some distributions the command is grub-mkrescue; use whichever is available.Testing the ISO with QEMUqemu-system-x86_64 -m 4096 -smp 4 -cdrom custom_os.iso -boot dYou should see a simple prompt provided by BusyBox:"
                        } ,
                     
                        {
                          "title"    : "C++ Derin Öğrenme-3 Pytorch C++ API LibTorch Modelleri Çalıştırma",
                          "category" : "",
                          "tags"     : " Deep Learning, Derin Öğrenme, PyTorch, LibTorch, PyTorch C++ API, Machine Learning C++, Deep Learning C++, Machine Learning, Makine Öğrenmesi, Inference, Çıkarım",
                          "url"      : "/2020/12/13/libtorch-inference.html",
                          "date"     : "December 13, 2020",
                          "excerpt"  : "Yazı dizisinin bu yazısında LibTorch‘u kullanarak modelleri nasıl çalıştıracağımızı, çıkarım (inference) için nasıl kullanacağımızı göreceğiz. Yazı dizisinin ilk yazısında kullanım senaryolarını açıklarken Python‘da eğitilen modeli C++‘da çıkarım ...",
                          "content"  : "Yazı dizisinin bu yazısında LibTorch‘u kullanarak modelleri nasıl çalıştıracağımızı, çıkarım (inference) için nasıl kullanacağımızı göreceğiz. Yazı dizisinin ilk yazısında kullanım senaryolarını açıklarken Python‘da eğitilen modeli C++‘da çıkarım için kullanıp bazı darboğazları atlatabileceğinizi aktarmıştım. Şimdi Python‘da bir modeli eğitip (bununla zaman kaybetmemek için öneğitimli bir modeli kullanacağım), kaydedip C++‘da yükleyip çıkarım yapacağız.İlk önce Python‘da bir model eğittimizi ve modelin resnet152 nesnesinde tutulduğunu varsayalım. Dediğim gibi ben doğrudan öneğitimli bir model kullanacağım:import torchvision.models as modelsimport torchresnet152 = models.resnet152(pretrained=True)script = torch.jit.script(resnet152)traced = torch.jit.trace(resnet152, torch.rand(1, 3, 224, 224)) #Traced model için örnek girdi sağlanmalıdır.script.save(\"./model_zoo/resnet152_sc.pt\")traced.save(\"./model_zoo/resnet152_tr.pt\")Burada modeli torch‘un jit modülünü kullanarak hem script modunda hem de trace modunda kaydedelim. Python API‘da jit modülünün kullanımı hakkında bilgi sahibi değilseniz kısa bir ara verip dokümantasyonu okumanızı tavsiye ederim. Her ikisini de kaydetme nedenim ileride karşılaştırma için kullanacak olmamdır. Şimdi kaydedilen modelleri kullanarak C++‘da hızlıca çıkarım yapalım:#include &lt;torch/script.h&gt;#include &lt;iostream&gt;int main(int argc, const char* argv[]) {    if (argc != 2) {        std::cerr &lt;&lt; \"Usage: infer &lt;path-to-exported-script-module&gt;\n\";        return -1;    }    at::globalContext().setBenchmarkCuDNN(1);    torch::jit::script::Module module;    try {        // Deserialize the ScriptModule from a file.        module = torch::jit::load(argv[1]);        std::cout &lt;&lt; \"Module loaded successfully.\n\";        module.eval();    }    catch (const c10::Error&amp; e) {        std::cerr &lt;&lt; \"Error loading the model.\n\";        return -1;    }	    //RESNET input shape (BATCH_SIZE, 3, 224, 224)    const int BATCH_SIZE = 8, CHANNELS = 3, HEIGHT = 224, WIDTH = 224;     torch::NoGradGuard no_grad;    // Create a vector of inputs.    std::vector&lt;torch::jit::IValue&gt; inputs;    inputs.emplace_back(torch::ones({BATCH_SIZE, CHANNELS, HEIGHT, WIDTH}));    // Execute the model and turn its output into a tensor.    auto output = module.forward(inputs).toTensor();    for (int i = 0; i &lt; BATCH_SIZE; ++i) {        std::cout &lt;&lt; i &lt;&lt; \"th element class: \" &lt;&lt; torch::argmax(output.data()[i]).item&lt;long&gt;() &lt;&lt; \"\n\";    }	return 0;}Bu kodu derleyip çalıştıralım ve arkasından kodu inceleyelim:$ ./infer ./model_zoo/resnet152_sc.ptModule loaded successfully.Class of 0th element: 600Class of 1th element: 600Class of 2th element: 600Class of 3th element: 600Class of 4th element: 600Kodu en baştan inceleyecek olursak ilk olarak torch/script.h başlık dosyasını kodumuza dahil etmemiz yeterli olacaktır. Ardından torch::jit::script::Module sınıfının bir örneğini oluşturuyoruz. Bu aslında Python da kullandığımız torch.nn.Module sınıfıdır. Sonuç olarak model nesnesini taşır ve Python API’ın sağladığı özellikleri kullanmamızı sağlar:torch::jit::script::Module module;Ardından daha önce Python‘da kaydettiğimiz modelleri dosyadan okuyup bu nesneye aktarıyoruz. Kaydedilen modeli komut satırından argüman olarak alıp modeli okuyoruz ve hata yakalama mekanizmasını kullanıyoruz:    try {        // Deserialize the ScriptModule from a file.        module = torch::jit::load(argv[1]);        std::cout &lt;&lt; \"Module loaded successfully.\n\";    }    catch (const c10::Error&amp; e) {        std::cerr &lt;&lt; \"Error loading the model.\n\";        return -1;    }Burada model CPU üzerinde çalışacak şekilde kullanıyoruz. GPU kullanmak isterseniz aşağıdaki kodu modeli yüklediğiniz satırdan sonraki satıra eklemeniz yeterli olacaktır:module.to(at::kCUDA);Girdilerimizi modele beslemek için ihtiyacımız olan vektörü hazırlayacağız. torch::jit::script::Module sınıfının forward fonksiyonu bizden std::vector&lt;torch::jit::IValue&gt; türünden bir vektör bekliyor. Fonksiyon argümanı olan vektörü std::move() ile taşıma semantiğine uygun olarak kullandığından bellek kullanımı ve hız kaybını olası en düşük hale getirmeyi amaçladığını hatırlatmak isterim:std::vector&lt;torch::jit::IValue&gt; inputs;inputs.push_back(torch::ones({BATCH_SIZE, CHANNELS, HEIGHT, WIDTH}));//for GPU tensors://inputs.push_back(torch::ones({BATCH_SIZE, CHANNELS, HEIGHT, WIDTH}).to(at::kCUDA));torch::jit::IValue bir sınıf olarak tanımlanmıştır. Interpreter Value‘nun kısaltması olarak IValue sınıfı TorchScript interpreter tarafından desteklenen tüm temel türleri sarmalamaktadır. IValue sınıfı modellere girdi ve çıktılar için kullanılır. Bu sınıfın arayüzü oldukça geniş olmasına rağmen temel iki fonksiyonu için aşağıya bakabilirsiniz:///   // Make the IValuetorch::IValue my_ivalue(26);std::cout &lt;&lt; my_ivalue &lt;&lt; \"\n\";//////   // Unwrap the IValueint64_t my_int = my_ivalue.toInt() //toX() instead of X use appropriate data type for wrapped data.std::cout &lt;&lt; my_int &lt;&lt; \"\n\";Bu noktada neden böyle bir sınıfa ihtiyaç duyulduğuna gelecek olursak C++ tarafından sağlanan temel türlerden farklı olarak LibTorch‘un sağladığı türler farklıdır. Python API ile C++ API arasında uyumu sağlamaya yardımcı olduğundan öğrenme/kullanma/alışmayı kolaylaştırmaktadır.Artık son olarak modele girdileri besleyip çıktıda yığındaki (batch) her bir eleman en olası sınıfı standart çıktıya yazdırıyoruz:auto output = module.forward(inputs).toTensor();for (int i = 0; i &lt; BATCH_SIZE; ++i) {    std::cout &lt;&lt; \"Class of \" &lt;&lt; i &lt;&lt; \"th element: \" &lt;&lt; torch::argmax(output.data()[i]).item&lt;long&gt;() &lt;&lt; \"\n\";}Bu noktada bir karşılaştırma yapmakta fayda olacaktır. Bunun için Python’da jit modülünü kullanmadan, script ve trace modunda farklı yığın boyutları için CPU ve GPU’da kaydedilen modeli okuma ve çalışma süreleriyle bu işlemlerin C++’da yaptığımızda elde edeceğimiz süreleri (tüm testler 10 defa çalıştırılıp süre ortalama olarak hesaplanmıştır) karşılaştıracağız. İlk olarak kaydedilen model dosyalarının boyutlarına bakalım. Dosya boyutları arasında anlamlı bir fark bulunmadığını söyleyebiliriz:            Modül Tipi      Dosya Boyutu                  torch.nn.Module      241.6 MB              torch.jit.script      242 MB              torch.jit.trace      242.2 MB      Şimdi model dosyalarının diskten okunup CPU üzerinde modelin çalıştırılabilir hale getirilmesi için geçen süreye bakalım. Görüldüğü üzere öncelikle okumayı C++’da çok daha hızlı yapabiliyoruz:            CPU Okuma zamanı (ms)      Python      C++                  Script      0.985      0.449              Trace      0.912      0.356      Şimdi de model dosyasının okunması ve GPU üzerinde çalıştırılabilir hale getirilmesi için geçen süreye bakalım. Hız farkı azalsa da C++ hala daha hızlı okuyor ve modeli çalıştırılabilir hale getiriyor:            GPU Okuma zamanı (ms)      Python      C++                  Script      2.286      2.137              Trace      2.214      2.025      Artık yığın işleme sürelerini inceleyelim. Soldaki çizgede CPU, sağdaki çizgedeyse GPU üzerinde farklı yığın boyutlarında bir örnek için modelin çıkarım süreleri görülmektedir. Test durumlarının %95’inde değişen oranlarda C++ API’ı daha hızlı çalışmaktadır. CPU üzerinde genelde script modunda kaydedilen modeller, GPU tarafındaysa trace modunda kaydedilen modeller daha hızlı çalışmışlardır. Bu hesaplamalarda tek işlem/iş parçacığı kullanılmıştır:Sonuç olarak aslında C++ API kullanarak modelleri çalıştırmanın hiç de zahmetli olmadığını gördüğünüzü düşünüyorum. Aslında sıfırdan bir modeli C++ üzerinde geliştirmekte çok zor değil. Sonraki yazılarda bunu da açıklamaya çalışacağım. Ama öncelikle verilerle nasıl başa çıkacağımızı incelemenin daha iyi olacağını düşünüyorum. Sonraki yazıda verileri (resim, video, csv, metin vb.) okumayı ve LibTorch‘a çıkarım ve eğitim sırasında bu verileri nasıl sağlayacağımızı açıklayacağım.Dizinin diğer yazıları:      C++ Derin Öğrenme-1 Pytorch C++ API LibTorch Giriş        C++ Derin Öğrenme-2 Pytorch C++ API LibTorch Tensör İşlemleri  "
                        } ,
                     
                        {
                          "title"    : "C++ Derin Öğrenme-2 Pytorch C++ API LibTorch Tensör İşlemleri",
                          "category" : "",
                          "tags"     : " Deep Learning, Derin Öğrenme, PyTorch, LibTorch, PyTorch C++ API, Machine Learning C++, Deep Learning C++, Machine Learning, Makine Öğrenmesi, Tensors, Tensör İşlemleri",
                          "url"      : "/2020/12/06/libtorch-tensors.html",
                          "date"     : "December 6, 2020",
                          "excerpt"  : "Yazı dizisinin bu yazısında LibTorch‘da tensörlerin nasıl oluşturulduğunu, erişildiğini ve değiştirildiğini açıklayacağım. LibTorch‘un ne olduğunu, neler yapılabildiğini açıkladığım giriş niteliğindeki yazıyı okumadıysanız o yazıdan başlamanızı ta...",
                          "content"  : "Yazı dizisinin bu yazısında LibTorch‘da tensörlerin nasıl oluşturulduğunu, erişildiğini ve değiştirildiğini açıklayacağım. LibTorch‘un ne olduğunu, neler yapılabildiğini açıkladığım giriş niteliğindeki yazıyı okumadıysanız o yazıdan başlamanızı tavsiye ederim. Bu noktada bu yazı çok daha uzun olabilirdi ama olabilen tüm sadeliği ama yeterli bilgiyi sağlayacak şekilde süzüldüğünü ve ana referansın dokümantasyonun kendisi olduğunu unutmayın.ATen tensör kütüphanesi PyTorch‘un tensör işlemleri için  arka planda kullandığı kütüphanedir ve C++14 standartlarına uygun olarak yazılmıştır. Tensör tipleri dinamik olarak çözümlenmektedir. Sonuç olarak içinde tuttuğu veri tipi ne olursa olsun ya da CPU/GPU tensörü olursa olsun tek bir tensör arayüzü bizi karşılamaktadır. Tensör sınıfının arayüzünü dokümantasyondan inceleyebilirsiniz.Tensörler üzerinde işlem yapan yüzlerce fonksiyon bulunmaktadır. Bu fonksiyonların listesine bağlantıyı kullanarak ulaşabilirsiniz. Fonksiyon isimlendirmeleriyle ilgili olarak dikkatinizi çekmek istediğim bir nokta _ karakteriyle biten fonksiyonlar tensör üzerinde değişiklik yapmaktadırlar yani bu fonksiyon çağrılırken tensör C++’da sol taraf referansı (lvalue reference) olarak aktarılmaktadır. Şimdi bu sınıfı kullanmaya başlayalım:1. Tensör Oluşturma:1.1 Fabrika Fonksiyonlarını KullanmaBu fonksiyonlar Fabrika Tasarım Desenlerinde olduğu gibi çalışıp sonuçta torch::Tensor geri döndüren fonksiyonlardır. Aslında bunlardan bir tanesini ilk yazıda kullanmıştım: torch::rand() fonksiyonu argüman olarak aldığı şekile göre bize tensör geri döndürmektedir. Bu fonksiyonlar:  arange: Sıralı tamsayılardan oluşan tensör geri döndürür,  empty: İlk değer verilmemiş ,  eye: Returns an identity matrix,  full: Returns a tensor filled with a single value,  linspace: Returns a tensor with values linearly spaced in some interval,  logspace: Returns a tensor with values logarithmically spaced in some interval,  ones: Returns a tensor filled with all ones,  rand: Returns a tensor filled with values drawn from a uniform distribution on [0, 1).  randint: Returns a tensor with integers randomly drawn from an interval,  randn: Returns a tensor filled with values drawn from a unit normal distribution,  randperm: Returns a tensor filled with a random permutation of integers in some interval,  zeros: Returns a tensor filled with all zeros.Linkler Python dokümantasyonuna bağlantı vermektedir. C++ API’da ki işlevleri, parametreleri ve isimli argümanları aynıdır. İsimli argümanların torch::TensorOptions nesnesi vasıtasıyla tanımlanabildiğine, ulaşılabildiğine ve değiştirilebildiğine dikkat edin. Bunu birazdan torch:rand() fonksiyonunda ele alacağım ve diğer fonksiyonlarda da geçerli olacaklar.Şimdi kullanışlı fabrika fonksiyonlarına yakından bakalım (Not: İlk fonksiyonu detaylı inceleyeceğim, diğerleri için dokümantasyonu kullanmanızı önereceğim. Çünkü API’lar hızlı değişime uğruyor ve senkron tutmak zahmetli olacaktır.):1.1.1. torch::rand()Bu fonksiyon [0,1) aralığında rastgele kayan noktalı sayılar üretir. Fonksiyonun protipine bakalım: torch::rand(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor.Parametreleri  size (int): Tensörün şeklini belirlediğimiz parametredir. Tamsayı değerler alır.İsimli Argümanları  out (Tensor, optional) – Çıktı tensörü.  dtype (torch.dtype, optional) – Tensör veri tipi.  layout (torch.layout, optional) – Tensörün bellekte tutulma yöntemi (dense/strided). Bu seçeneğin ileride ki versiyonlarda kaldırılması planlanmaktadır.  device (torch.device, optional) – Tensörün hangi aygıtta saklanacağı (CPU/GPU).  requires_grad (bool, optional) – Geri döndürülen tensörün otomatik gradyan işlemine tabii olup olmayacağı.Kullanımı:En basit kullanımı şekli verip tensörü kullanmaktır.torch::Tensor randTensor = torch::rand(/*size:*/{2, 3});Bu kullanım ilk yazıdan hatırlarsanız şekli 2, 3 olan bir CPU tensörü geri döndürmüştü. Hatırlayacak olursak bu tensörü standart çıkış akımına yazdırırsak aşağıdaki çıktıyı alırız (çıktılardaki kayan noktalı sayıların rastgele olacağını unutmayın):0.6147  0.6752  0.89630.5627  0.4836  0.5589[ CPUFloatType{2,3} ]İsimli argümanların kullanımı için önce bu seçenekleri bir torch::TensorOptions nesnesi içerisinde tanımlamamız gerekiyor. Bu seçeneklerin diğer fabrika fonksiyonlarında da aynı şekilde kullanıldığını hatırlatıp alabilecekleri değerlere göz atalım:  dtype: kUInt8, kInt8, kInt16, kInt32, kInt64, kFloat32 ve kFloat64,  layout: kStrided ve kSparse,  device: kCPU ya da kCUDA (birden fazla GPU’nuz varsa aygıt indeksi de alır),  requires_grad: true ya da false.Şimdi seçenekleri kullanarak bir tensör oluşturalım. 32 bitlik kayan noktalı sayıları strided bellek yerleşiminde, 0 numaralı GPU’da otomatik gradyana dahil olacak bir tensör elde etmek için aşağıdaki kod öbeğini kullanabiliriz:auto options = torch::TensorOptions()            .dtype(torch::kFloat32)            .layout(torch::kStrided)            .device(torch::kCUDA, 0)            .requires_grad(true);torch::Tensor randTensor = torch::rand(/*size:*/{2, 3}, options);Bu seçeneklerden bir veya birkaçını doğrudan fonksiyon olarak da kullanabilirsiniz. Bu fonksiyonlar bekleneceği üzere torch::TensorOptions nesnesi geri döndürür :torch::Tensor randTensor = torch::rand(/*size:*/{2, 3}, torch::TensorOptions().dtype(torch::kFloat32));/* veya torch::Tensor randTensor = torch::rand({2, 3}, torch::dtype(torch::kFloat32));torch::Tensor randTensor = torch::rand({2, 3}, torch::dtype(torch::kFloat32).device(torch::kCUDA, 0));*/1.1.2. torch::randint()Bu fonksiyon verilen iki değer arasında düzgün dağılıma uygun şekilde tamsayılardan oluşan bir tensör geri döndürür. Hemen kullanımına bakalım:auto intTensor = torch::randint(/*low:*/1, /*high:*/9, /*size:*/{3});Yukarıdaki tanımlamayla 1 ile 9 arasında 3 elemanı olan bir vektör olacak bir tensör oluşturmuş oluyoruz. Bu tensörün çıktısına bakarsak: 6 1 1[ CPUFloatType{3} ]3B bir tensör (tipik olarak bir görüntü verisini taşıyan bir tensör olarak düşünebilirsiniz) oluşturalım:auto intTensor = torch::randint(/*low:*/1, /*high:*/9, /*size:*/{1920, 1080, 3});1.1.3. torch::ones()/torch::zeros()Bu fonksiyonlar isimlerinden anlaşıldığı üzere birler veya sıfırlardan oluşan tensörler oluşturmanızı sağlarlar. Kullanımları yine diğer fonksiyonlarla benzerdir. Örneğin:auto onesTensor = torch::ones(/*size:*/{5, 10});auto zerosTensor = torch::zeros(/*size:*/{1, 5, 10});1.1.4. torch::from_blob()Çoğunlukla tensörü oluşturacak veriyi başka bir kaynaktan okuyupo bu tensöre aktarırız. Bunu yapabilmek için void* olarak veriyi alan ve tensör geri döndüren kullanışlı bir fonksiyon bulunmaktadır.:float data[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0 8.0, 9.0, 10.0};auto blobData = torch::from_blob(data, /*size:*/{2, 5});/*  1   2   3   4   5  6   7   8   9  10[ CPUFloatType{2,5} ]*/Bu fonksiyonun argümanına gönderilen verinin sahipliğini almaz. Ancak tensör nesnesinin ömrü bittiğinde özgün nesneyi de bellekten siler.  Yine isterseniz tensör seçeneklerini kullanabilirsiniz.auto blobDataD = torch::from_blob(data, {1, 10}, torch::requires_grad(False));1.1.5. torch::tensor fonksiyonuSınıfın kurucu işlevlerini doğrudan kullanan bu fonksiyonla da tensör oluşturmak söz konusudur.auto tensorInit = torch::tensor({1.0, 2.0, 4.0, 2.0, 3.0, 5.0});Diğer fabrika fonksiyonlarına dokümantasyona bırakıp şimdi torch::Tensor sınıfının üye fonksiyonlarını inceleyelim.2. torch::Tensor Sınıf Üye FonksiyonlarıArtık elimizde tensörümüz var ve onun hakkında bilgi almak/değişiklik yapmak istiyoruz. Burada torch::Tensor sınıfının bize sağladığı sınıf üye fonksiyonlarından en önemlilerine göz atalım:// 1 boyutlu bir tensör oluşturalımauto tensorInit = torch::tensor({1.0, 2.0, 4.0, 2.0, 3.0, 5.0, 1.0, 2.0, 4.0, 2.0, 3.0, 5.0});// 2B tensöre dönüştürelim tensorInit = tensorInit.reshape({2,6});// dim() sınıf üye fonksiyonu tensörün kaç boyutlu olduğunu geri döndürür. Örneğimizde 2 olarak:auto tDims = tensorInit.dim();// dtype() sınıf üye fonksiyonu tensörün veri tipini geri döndürür. Örneğimizde float olarak:auto tDtype = tensorInit.dtype();// sizes() sınıf üye fonksiyonu tensörün tuttuğu verinin şeklini geri döndürür. Örneğimizde  [2, 6] olarak:auto f = tensorInit.sizes();3. Tensör Elemanlarına Erişim, İndeksleme ve DeğiştirmeTensör elemanlarına erişim için birden fazla yöntem bulunmaktadır. Öncelikle torch::Tensor sınıf üye fonksiyonlarından bir tanesi torch::Tensor.data_ptr() fonksiyonunu kullanarak tüm veriye erişebiliriz. Alternatif olarak data()fonksiyonunu kullanıp Python‘da olduğu gibi bir elemanına erişmek mümkündür.auto tensorInit = torch::tensor({1.0, 2.0, 4.0, 2.0, 3.0, 5.0, 11.0, 12.0, 14.0, 12.0, 13.0, 15.0});tensorInit = tensorInit.reshape({2,6});// void* türünden bir gösterici (pointer) geri döndürürauto pDataVoid  = tensorInit.data_ptr();// veri tipinin float olduğu varsayımıyla dönüştürüp göstericiyi kullanarak elemanlara ulaşabiliriz. auto pDataFloat = static_cast&lt;float*&gt;(pDataVoid);std::cout &lt;&lt; pDataFloat[7] &lt;&lt; \"\n\"; // İki boyutta [1][1]  tek boyutta 7. eleman yani 12// Alternatif olarak // Doğrudan 1,0 indeksinde bulunan veriye data fonksiyonu ile de ulaşabiliriz.std::cout &lt;&lt; tensorInit.data()[1][1] &lt;&lt;\"\n\" ; // örnekte 12LibTorch ile veriye erişim için torch::Tensor kütüphanesinin sunduğu bir diğer alternatif ve tavsiye edilen yöntemse accessor kullanımıdır. Burada CPU ve GPU için ayrı accessor kullanmak gerekmektedir. Önce bir CPU tensörü için ardından da GPU tensörü için bu işlemi kullanalım:auto tensorInit = torch::tensor({1.0, 2.0, 4.0, 2.0, 3.0, 5.0, 11.0, 12.0, 14.0, 12.0, 13.0, 15.0});tensorInit = tensorInit.reshape({2,6});auto tensorInitAccessor = tensorInit.accessor&lt;float, 2&gt;();for (int i = 0; i &lt; tensorInitAccessor.size(0); i++)    for (int j = 0; j &lt; tensorInitAccessor.size(1); j++) {        std::cout &lt;&lt; \"Data in position \" &lt;&lt; i &lt;&lt; \"-\" &lt;&lt; j &lt;&lt; \": \" &lt;&lt; tensorInitAccessor[i][j] &lt;&lt; \"\n\";    }Bu kullanımda dikkat edilecek husus accessor nesnenin şablon parametrelerine veri tipi olan float ve boyutu gösteren 2 göndermemiz gerektiğidir. Yani farklı veri tipi ve boyutlar için bir özelleşme gerekecektir. Yani işleri çokta kısalttığını söyleyemem. Dokümantasyonun iddiası daha hızlı erişim sağladığı yönünde ama gösterici kullanımının yavaş olmasını pek mümkün görmediğim için büyük bir tensörde tüm elemanlara ulaşmayı test etmek istedim:#include &lt;iostream&gt;#include &lt;torch/torch.h&gt;#include &lt;chrono&gt;int main() {    using namespace std::chrono;        const int HEIGHT = 1920, WIDTH = 1080, CH = 300;    long double sum = 0;        auto randBigTensor = torch::rand({HEIGHT, WIDTH, CH});        auto start = high_resolution_clock::now();    auto tensorInitAccessor = randBigTensor.accessor&lt;float, 3&gt;();    for (int i = 0; i &lt; tensorInitAccessor.size(0); i++)        for (int j = 0; j &lt; tensorInitAccessor.size(1); j++)            for (int k = 0; k &lt; tensorInitAccessor.size(2); k++) {                sum += (tensorInitAccessor[i][j][k]) / 1000;            }    auto end = high_resolution_clock::now();    duration&lt;double&gt; time_span = duration_cast&lt;duration&lt;double&gt;&gt;(end - start);    std::cout &lt;&lt; \"It took me \" &lt;&lt; time_span.count() &lt;&lt; \" seconds (Using accesscor). Sum = \" &lt;&lt; sum &lt;&lt; \"\n\";    sum = 0;    start = high_resolution_clock::now();    auto pDataVoid = randBigTensor.data_ptr();    auto pDataFloat = static_cast&lt;float *&gt;(pDataVoid);    for (int i = 0; i &lt; (HEIGHT * WIDTH * CH); i++)        sum += (pDataFloat[i]) / 1000;    end = high_resolution_clock::now();    time_span = duration_cast&lt;duration&lt;double&gt;&gt;(end - start);    std::cout &lt;&lt; \"It took me \" &lt;&lt; time_span.count() &lt;&lt; \" seconds (Using data_ptr). Sum = \" &lt;&lt; sum &lt;&lt; \"\n\";        return 0;}Sonuçlar aslında beklediğim gibi oldu. En hızlısı veri göstericiyi alıp veriye erişmek oldu. Ama bu testi farklı boyutlarda hatta ideali uygulamanızda kullanacağınız tensör boyutlarıyla test etmek daha mantıklı olabileceğini düşünüyorum.It took me 18.573 seconds (Using accesscor). Sum = 311033It took me 4.24126 seconds (Using data_ptr). Sum = 311033Bazen bu verinin belirli elemanlarına erişmek isteyebiliriz. Bu durumda Python‘dan alışık olduğumuz Indexing API kullanımı daha kolay olacaktır. Bu API’de hem okuma hem de yazma işlemi yapmak mümkündür:auto randTensor = torch::rand({100, 100, 3});//1,0,5 konumundaki elemanstd::cout &lt;&lt; randTensor.index({1,0,5}); // Slice fonksiyonuyla 0.boyuttaki verileri 1.indeksten başlayıp 10'a kadar 2'şer artan şekilde alır ve diğer iki eksende 0. indekse denk gelen elemanları gösterir. Burada `torch::indexing` isim alanının eklendiğine dikkat edin.  using namespace torch::indexing;std::cout &lt;&lt; randTensor.index({Slice(/*start_idx:*/1, /*stop_idx:*/10, /*step:*/2), 0, 0})}); //1,0,5 konumundaki elemana değer atamarandTensor.index({1,0,5}) = 0.05Indexing API’nin Python ve C++ için kullanımının kıyaslaması için bağlantıya tıklayın.4. Dönüşüm İşlemleriBu yazıyı bitirmeden önce son olarak tensörlerin dönüşüm işlemlerine bakalım.  Burada ilk oluşturma esnasında belirlediğimiz tensör seçeneklerini dönüştürmek mümkündür.auto sourceTensor = torch::randn({2, 3}, torch::kFloat16);// Veri tipini dönüştürmeauto floatTensor32 = sourceTensor.to(torch::kFloat32);// Aygıt tipine göre dönüştürmeauto gpuTensor = floatTensor32.to(torch::kCUDA);Evet yazının sonuna geldik. Yazı dizisinin bir sonraki yazısında artık modellere doğru geçiş yapacağımız yazı hazır olduğunda bu yazının altına bağlantıyı ekleyeceğim.Dizinin diğer yazıları:      C++ Derin Öğrenme-1 Pytorch C++ API LibTorch Giriş        C++ Derin Öğrenme-3 Pytorch C++ API LibTorch Modelleri Çalıştırma  "
                        } ,
                     
                        {
                          "title"    : "C++ Derin Öğrenme-1 Pytorch C++ API LibTorch Giriş",
                          "category" : "",
                          "tags"     : " Deep Learning, Derin Öğrenme, PyTorch, LibTorch, PyTorch C++ API, Machine Learning C++, Deep Learning C++, Machine Learning, Makine Öğrenmesi",
                          "url"      : "/2020/12/03/libtorch-config.html",
                          "date"     : "December 3, 2020",
                          "excerpt"  : "Çoğumuz farklı tartışma ortamlarında Makine Öğrenmesi (Machine Learning) ya da popüler alt alanı Derin Öğrenme (Deep Learning) için en iyi dilin (genelde buna bir de en iyi ML kütüphanesi tartışmaları eklenmektedir) hangisi olduğu konusunda çeşitl...",
                          "content"  : "Çoğumuz farklı tartışma ortamlarında Makine Öğrenmesi (Machine Learning) ya da popüler alt alanı Derin Öğrenme (Deep Learning) için en iyi dilin (genelde buna bir de en iyi ML kütüphanesi tartışmaları eklenmektedir) hangisi olduğu konusunda çeşitli tartışmalara denk gelmişizdir. Henüz gelmemişseniz çok heveslenmeyin yakında denk geleceksinizdir. Bu soruya verilecek en doğru cevap bence “duruma göre değişir” olabilir. Ayrıca Elon Musk’ın 2 Şubat 2020’de attığı tweet bunun güzel bir göstergesi oldu.Bu noktada bu konuda daha fazla laf kalabalığı yapmak yerine bu yazıda neler bulabileceğinize geçeceğim. Yazının bölümleri:1. GirişÖzellikle üretim aşamasında düşük gecikme süreli/yakın gerçek zamanlı modeller kullanmak istediğinizde karşınıza bir kaç sorun çıkmaktadır. Karşılaşılan en kritik sorunların başında Python‘da yaşanan (yanlış anlaşılması dilin daha farklı bir iddiası yok) hız/gecikme sorunları ve çoklu işlem/iş parçacığı kullanımı (nesnelere erişimde Global Interpreter Lock tarafından kısıtlanan hususları kastediyorum) kaynaklı sorunların geldiğini düşünüyorum. Bu noktada C++ gibi bir dilin getireceği bazı kolaylıkları kullanmak isteyebilirsiniz. İleride bu noktada ONNX, TensorRT gibi kütüphanelerin nasıl işe dahil edilebileceğini açıklayan yazılar yazmayı düşünüyorum. Ama bu yazıda PyTorch tarafından geliştiricilere sağlanan ve öğrenme eğrisi çok da dik olmayan LibTorch kütüphanesinden bahsedeceğim. Giriş niteliğindeki bu yazıyı tensör işlemleri, çıkarım (inference) yapma ve bir modeli sıfırdan C++’da oluşturmayı/eğitmeyi anlatan ayrı yazılar takip edecek.LibTorch kütüphanesi PyTorch 1.0 versiyonu ile hayatımıza giren PyTorch‘un C++ API‘sidir. Facebook tarafından hem araştırma hem de üretimde kullanılmaktadır. Python tarafında kullanılan tüm özellikler aşağı yukarı C++ API‘sinde de bulunmaktadır. Ancak biraz geriden de gelse genelde 1 versiyon farkla PyTorch‘da kullanılan özeliklerin C++ API‘sine eklendiğini gözlemliyorum. Kendi dokümantasyonunda bu özelliği “beta” olarak değerlendirmemiz gerektiği ve PyTorch‘un Python arayüzünün daha stabil olduğu konuları vurgulanmaktadır. Ancak ben uzun bir süredir kullanıyorum ve önemli hiçbir sorun yaşamadığımı belirtmek isterim.Peki LibTorch bize neler sunuyor:  Makine öğrenmesi modelleri tanımlamak için bir arayüz (Python’da torch.nn.Module karşılığı),  En yaygın modüller (evrişim, yinelemeli ağlar, yığın normalleştirme vb.) için standart bir kütüphane,  Eniyileme API (SGD, Adam vb.),  Veri setlerini ve veri iletim hatlarınızı temsil eden ve CPU çekirdeklerinde paralel işleme kabiliyeti,  Modellerin otomatik olarak GPU’da paralelleştirme kabiliyeti (torch.nn.parallel.DataParallel),  C++ modellerini kolayca Python’da kullanabilme,  TorchScript JIT derleyici kullanımı,  ATen (temel tensör arayüzü) ve Autograd API (hesaplamalı çizge üzerinde otomatik olarak gradyanları hesaplayan API) sunmaktadır.Sunduğu bileşenlerin listesine ve tanımlarına dokümantasyonundan ulaşabilirsiniz.2. Kullanım SenaryolarıPeki C++’da LibTorch kullanım senaryoları neler olabilir?  Bence öncelikle üretim aşamasında tamamen Python‘da geliştirilmiş, eğitilmiş ve kaydedilmiş modelin (jit scripted/traced) okunup doğrudan çıkarım işlemi yapılabilir. Bu arada çoklu işlem/iş parçacığı kullanımı, düşük gecikme süresi gibi avantajları kullanabilirsiniz. Bu yazı dizisinin 3. yazısında doğrudan çıkarım için kullanımı ele alacağım. Elbette o yazıda farklı karşılaştırmalar bulabileceksiniz.  Model sıfırdan C++ da oluşturulabilir/eğitilebilir.  Yazı dizisinin 4. yazısında modeli C++‘da modeli sıfırdan yazıp eğiteceğim.  PyTorch üzerinde kendi eklentilerinizi yazabilirsiniz.3. KurulumKurulum konusunda iki seçenek söz konusudur:      Derlenmiş kütüphaneyi indirmek:                  Bağlantıya tıklayın.                    Önce Stable linkine, daha sonra kullandığınız işletim sistemi linkine, daha sonra LibTorch linkine, ardından C++/Java linkine tıklayıp son olarak GPU için CUDA versiyon linkine eğer sadece CPU kullanacaksanız None linkine tıklayıp gelen dosya bağlantısı indirip dilediğiniz bir dizine içeriğini kopyalayın.                          Kaynak kodundan derlemek: Biraz zahmetli olabilir (aslında epey zahmetli oluyor). Ama kişisel olarak benim tercihim bu seçenektir. GitHub reposunu klonlayıp gerekli adımları uygulayın (Bu bölüm çok uzamaması için ben atlıyorum ama ileride bir yazıyı buna ayırabilirim).  4. Hello LibTorchBenim bu yazı dizisinde anlatacaklarım Linux işletim sisteminde geliştirme editörü olarak Clion ve cmake aracını kullanmayı içerecek ama kendi işletim sisteminiz ve derleme aracınıza aynı işlem maddelerini aktarabilirsiniz. Hadi başlayalım ve öncelik bir CMakeLists.txt isimli bir dosya oluşturup aşağıdaki içeriği ekleyelim:cmake_minimum_required(VERSION 3.17) #cmake versiyonu en düşük 3.0 olduğu sürece mevcut cmake kurulumunuzu kullanabilirsinizproject(libtorchHelloWorld)set(CMAKE_CXX_STANDARD 14)find_package(Torch 1.7.0 REQUIRED) # Şu anda mevcut sürüm 1.7.0 farklı bir versiyon kullanırsanız burayı düzeltmelisinizset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\")add_executable(libtorchHelloWorld main.cpp) target_link_libraries(libtorchHelloWorld ${TORCH_LIBRARIES})Artık main.cpp içerisinde kaynak kodlarımızı ekleyelim:#include &lt;iostream&gt;#include &lt;torch/torch.h&gt;int main() {    torch::Tensor randTensor = torch::rand({2, 3});        std::cout &lt;&lt; \"Hello LibTorch\n\" &lt;&lt; \"Torch Tensor: \" &lt;&lt; randTensor &lt;&lt; \"\n\";}Ve derleme çıktılarını içerisinde barındıracak bir dizin oluşturup içerisinde derlemeyi başlatalım (-DCMAKE_PREFIX_PATH=/absolute/path/libtorch ile belirttiğiniz yol kurulum bölümündeki içeriği kopyaladığınız dizin olmalı):$ mkdir build$ cd build$ cmake -DCMAKE_PREFIX_PATH=/absolute/path/libtorch ..$ cmake --build . --config DebugArtık derlenen dosyamızı çalıştırıp Hello World klasiğini yerine getirelim:$ ./libtorchHelloWorldHello LibtorchTorch Tensor:  0.6147  0.6752  0.8963 0.5627  0.4836  0.5589[ CPUFloatType{2,3} ]Şimdi şu kısacık uygulamaya bakarak API’ye biraz yakından bakalım. İlk olarak torch/torch.h başlık dosyası LibTorch‘un tüm diğer başlık dosyalarını içeren torch/all.h başlık dosyasını içerir:#pragma once#include &lt;torch/all.h&gt;#ifdef TORCH_API_INCLUDE_EXTENSION_H#include &lt;torch/extension.h&gt;#endif // defined(TORCH_API_INCLUDE_EXTENSION_H)torch/all.h başlık dosyasını incelersek gerekli tüm başlık dosyalarına sahip olduğumuzu görürüz. Bu nokta da torch/torch.h başlık dosyasından başka bir başlık dosyasına ihtiyacımız olmadığını görmüş olduk:#pragma once#include &lt;torch/cuda.h&gt;#include &lt;torch/data.h&gt;#include &lt;torch/enum.h&gt;#include &lt;torch/jit.h&gt;#include &lt;torch/nn.h&gt;#include &lt;torch/optim.h&gt;#include &lt;torch/serialize.h&gt;#include &lt;torch/types.h&gt;#include &lt;torch/utils.h&gt;#include &lt;torch/autograd.h&gt;Aslında bir ipucu olarak PyTorch kullanırken kullandığınız . operatörünü :: ile değiştirdiğinizde çoğunlukla C++ API için geçerli bir kod yazmış olursunuz dersem çok da yanıltıcı olmaz diye düşünüyorum. Dolayısıyla main fonksiyonu içerisinde tanımladığımız randTensor isimli değişken Python API‘den alışık olduğumuz torch.tensor sınıfının bir örneğidir.  Daha sonra bu sınıfı daha ayrıntılı inceleyeceğim.	torch::Tensor randTensor = torch::rand({2, 3});Yukarıdaki kod ile torch isim alanı içerisinde tanımlı rand fonksiyonun farklı yüklemelerinden birisini kullanarak 2, 3 şekline sahip rastgele değerlerle doldurulmuş bir tensor elde etmiş oluyoruz.	std::cout &lt;&lt; \"Hello LibTorch\n\" &lt;&lt; \"Torch Tensor: \" &lt;&lt; randTensor &lt;&lt; \"\n\";Yukarıdaki kod satırındaysa randTensor &lt;&lt; operatörünün sağ operandı olduğunda  at isim alanı (bu isim alanı LibTorch ve PyTorch tarafından kullanılan temel tensör kütüphanesi olan ATen kütüphanesini içermektedir) içerisindeki print fonksiyonunu çağırır ve standart çıktıya tensör verisini, veri tipini ve şeklini yazdırır. torch::Tensor  sınıfının aynı Python API da olduğu sağladığı çok fazla şey var. Ama onlar bir sonraki yazıda yer alacak. Bu yazının sonuna ulaştık. Bu yazıyı giriş bölümünde de belirttiğim gibi bir dizi yazı takip edecek. Yeni bölümleri yazdıktan sonra bu yazının altına bağlantılarını ekleyeceğim.Dizinin diğer yazıları:      C++ Derin Öğrenme-2 Pytorch C++ API LibTorch Tensör İşlemleri        C++ Derin Öğrenme-3 Pytorch C++ API LibTorch Modelleri Çalıştırma  "
                        } ,
                     
                        {
                          "title"    : "NVIDIA Docker Kurulumu ve Derin Öğrenme için Kullanımı",
                          "category" : "",
                          "tags"     : " NVIDIA-Docker, Docker, Docker Temel Bilgiler, Docker Kurulumu, DockerFile, Deep Learning, Derin Öğrenme, Tensorflow",
                          "url"      : "/2020/04/22/nvidia-docker-usage.html",
                          "date"     : "April 22, 2020",
                          "excerpt"  : "22.04.2020 tarihinde güncellenmiştir.Derin Öğrenme (Deep Learning) ile uğraşmaya başlayan herkesin bir şekilde korkulu rüyası maalesef gerekli paketlerin, araçların kurulması ve birbirlerinin gereksinimleri ile uyumsuzluk yaratmadan çalışabilmesi ...",
                          "content"  : "22.04.2020 tarihinde güncellenmiştir.Derin Öğrenme (Deep Learning) ile uğraşmaya başlayan herkesin bir şekilde korkulu rüyası maalesef gerekli paketlerin, araçların kurulması ve birbirlerinin gereksinimleri ile uyumsuzluk yaratmadan çalışabilmesi olmuştur. Hele bir de aynı anda farklı projelerle çalışıyorsanız bunların hepsinin ayrı gereksinimleri varsa işler daha da sorunlu olmaktadır. Bunun için Python paket yönetimi ile sunulan virtualenv virtualenv, virtualenvwrapper kullanımı belli oranda işe yarasa bile bazen sorun, geliştirme ortamınızda mevcut ekran kartının(NVIDIA Cuda çekirdeğine sahip olan) CUDA sürücüleri, cuDNN kütüphanesi kurmaya çalıştığınız paketler ile uyumlu olmaması/olamaması nedeniyle kurulum işlemleri gerekenden daha fazla zaman harcamamıza neden olabilmektedir. Bunların hepsi bir araya getirilse bile bu sefer de sisteminizde yaptığınız bir işletim sistemi/donanım sürücüsü güncellemesi bütün emeklerin çöpe gitmesi anlamına gelebilmektedir.Bu noktada daha fazla izole/sanal ortama ihtiyaç ortaya çıkmaktadır. Sorunların ortadan kaldırılmasında Docker etkin bir çözüm olarak karşımıza çıkmakta ve giderek daha çok geliştirici tarafından tercih edilmektedir. Dahası ekran kartının hesap gücünden faydalanmak isteyen kullanıcıların yardımına bir de NVIDIA-Docker koşmaktadır. Bu yazımızda çok fazla teknik ayrıntısına boğulmadan gerekli kavramları öğrenerek bu çözümü derin/makina öğrenmesi geliştiricileri için nasıl faydalı bir şekilde kullanılabileceği üzerine odaklanacağız. Teknik ayrıntılar için Gökhan Şengün tarafından kaleme alınan yazıya/yazılara başvurabilirsiniz.  Konuyu derin öğrenme özelinde anlatmaya çalışacağımı tekrar hatırlatarak özellikle İngilizce kaynak sayısı oldukça fazla olsa da Türkçe kaynak bulmakta sorun yaşanmakta olduğunu değerlendirdiğim için de bu yazıyı Türkçe olarak paylaşıyorum.Ana Başlıklar:  Temel Kavramlar  Docker, NVIDIA Docker Kurulumu  Hazır görüntülerin(image) kullanımı  DockerFile ile özgün görüntülerin kullanılması  Komut satırı üzerinden Docker ile etkileşim1. Temel Kavramlar:Docker nedir?Kendi başına çalışabilen, ihtiyaç duyduğu herşeyi (sistem araçları, sistem kütüphaneleri, gerekli paketler, donanım sürücüleri vb.) kendi içinde bulunduran, hafif bir yazılımdır. Dahası içerisinde barındırdığı tüm bileşenleri aynı makine üstünde (ana makine-host) çok farklı ayarlarla istediğiniz sayıda görüntüyü (image) farklı konteyner (container) içinde çalıştırmak mümkündür. Sonuç olarak; geliştirici için normal şartlarda ayrı ayrı sahip olmak veya ayarlamak çok maliyetli ve zahmetli olabilecek süreçler hem çok ekonomik hem de çok süratli olabilmektedir.Görüntü (Image)İçerisinde işletim sistemi NVIDIA sürücülerini ve gerekli tüm araç, paket ve programları barındıran yapıdır. Docker kurulumunu anlattıktan sonra ana makinede mevcut görüntülerin (image) nasıl oluşturulacağını, görüntüleneceğini ve yapılabilecek işlemlere değineceğiz.Konteyner (Container)Docker görüntüsünün üzerinde koştuğu izole/sanal ortamdır. Konteyner üzerinde yapılabilecek işlemlere ileride değineceğiz.2. Docker, NVIDIA Docker Kurulumu:Docker CE sürümünün kurulum yönergelerine bağlantı üzerinden ulaşabilirsiniz. Ben size Ubuntu bash terminal üzerinde kurulumunu göstereceğim.  İlk önce daha önce kurulan Docker CE sürümünü apt ile kaldırıyoruz.$ sudo apt-get remove docker docker-engine docker.io  Daha sonra Docker CE kurulumuna geçiyoruz ve aşağıdaki komutları sıra ile terminalden uyguluyoruz.apt paket endekslerini güncelliyoruz.$ sudo apt-get updateapt ile gerekli paketleri kuruyoruz.$ sudo apt-get install \    apt-transport-https \    ca-certificates \    curl \    software-properties-commonDocker’ın resmi GPG anahtarını kendi anahtar zincirimize ekliyoruz.$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -stable olarak işaretlenmiş paketleri kurmak için depomuza ekliyoruz.$ sudo add-apt-repository \   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \   $(lsb_release -cs) \   stable\"apt paket endekslerini tekrar güncelliyoruz.$ sudo apt-get updateVe sonunda Docker CE’nin son sürümünü kuruyoruz.$ sudo apt-get install docker-ce“Merhaba Dünya“sız yapamazdık. Aşağıdaki komut ile henüz bilgisayarımızda olmayan hello-world isimli bir görüntüyü DockerHub adı verilen geliştiricilerin ve resmi olarak kullanılan görüntülerin paylaşıldığı bir çeşit uygulama dükkanından indirip çalıştırıyoruz ve terminal standart çıktısında aşağıdaki çıktıyı görüyoruz.$ sudo docker run hello-worldUnable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-world9bb5a5d4561a: Pull complete Digest: sha256:f5233545e43561214ca4891fd1157e1c3c563316ed8e237750d59bde73361e77Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.    (amd64) 3. The Docker daemon created a new container from that image which runs the    executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it    to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/engine/userguide/Docker kurma işlemimiz bitti. Şimdi de derin öğrenme (deep learning) modellerini eğitme işlemimizi kısaltacak önemli bir donanım olan ekran kartı üreticisi NVIDIA’nın hayatımıza soktuğu nimetlerden faydalanmak için bir de NVIDIA-Docker kurmaya başlayabiliriz. Bu noktada ana makinemizde (host) NVIDIA ekran kartı (CUDA yeteneğine sahip) ve ekran kartı sürücüsü kurulmuş olması gerekmektedir.Önce işletim sistemi dağıtımızı bir eçvre değişkenine yazıyoruz.$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID)Sonra anahtar zincirimize resmi GPG anahtarını ekliyoruz.$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -Şimdide uygulama kaynaklarımıza nvidia dağıtımlarını ekliyoruz.$ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.listNVIDIA-Docker kurulumunu yapıyoruz ve docker servisini yeniden başlatıyoruz.$ sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit$ sudo systemctl restart dockerYine kurulumumuzu test etmek için bu sefer NVIDIA’ya ait son CUDA deposunu kendi bilgisayarımıza indirip herhangi bir sorun olmadığına emin oluyoruz. Bu noktada sizin ekran kartı modeli, sürücüsü ve özellikleri ile uyumlu olarak standart çıktıda aşağıdakine benzer bir sonuç alıyoruz.$ docker run --gpus all --rm nvidia/cuda nvidia-smiSun May 20 18:33:05 2018       +-----------------------------------------------------------------------------+| NVIDIA-SMI 384.111                Driver Version: 384.111                   ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  GeForce GTX 1070    Off  | 00000000:0A:00.0  On |                  N/A ||  0%   49C    P8    11W / 200W |    359MiB /  8110MiB |      7%      Default |+-------------------------------+----------------------+----------------------+                                                                               +-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================|+-----------------------------------------------------------------------------+3. Hazır Görüntülerin(image) Kullanımı:DockerHub üzerinden paylaşılmış hazır görüntülere (image) ulaşabilirsiniz. Kolaydan başlayarak zora doğru gideceğimiz için önce hazır depoları kullanacağız. Ben size Tensorflow’un resmi deposundan son sürümünü nasıl kuracağınızı göstereceğim.Terminal üzerinden aşağıdaki komutu verdiğimizde uzak depo alanından tensorflow/tensorflow isimli deponun son sürümünü(latest-gpu) ana makinemize çekip (pull) etkileşimli modda çalıştırıp (-p) parametresi ile dış dünya ile 8888 nolu portdan haberleşmesini söylüyoruz. Daha sonra localhost:8888 üzerinden çalışan Jupyter Notebook karşımıza çıkıyor. Bundan sonra bu komutu her çalıştırdığımızda Docker, uzak depo yerel makinemize bulunduğu için indirmek yerine doğrudan çalıştırmaya başlayacaktır.$ docker run -it --gpus all -p 8888:8888 tensorflow/tensorflow:latest-gpu# NVIDIA ekran kartı olmayanlar için docker run -it -p 8888:8888 tensorflow/tensorflow ile kurulum yapılabilir.DockerHub üzerinden ulaştığınız tüm depolarda farklı etiket (tag) varsa farklı sürümleri olduğunu düşünebilirsiniz. Gidip size uygun farklı sürümlerini de denemeniz mümkün olabilir. Görüldüğü gibi sadece parametreleri değiştirerek ana makinemiz (host) üzerinde bir çok farklı bilgisayar varmış gibi görüntüler (image) sayesinde istediğimiz özgürlüğe sahip oluyoruz.4. DockerFile ile Özgün Görüntülerin Kullanılması:Her zaman hazır bir depo kullanmak Docker’ın bize sunduğu esnekliği tam anlamıyla kullanmamıza imkan vermeyebilir. Bu noktada tamamıyla kendi istediğimiz bir görüntü oluşturmak gerekecektir. DockerFile bu eksikliği gidermek için kullanılan metin bazlı bir dosya olup içerisinde bulunan Docker’a özel sözdizim kuralları ile tam anlamıyla istediğimiz gibi bir görüntü oluşturmamıza yardım eder. Ben bu noktada DockerFile oluşturma konusunda yine Gökhan Şengün tarafından kaleme alınan yazıya mutlaka göz atmanızı tavsiye edip derin öğrenme merkezli olarak nasıl bir DockerFile kullanabileceğimize değineceğim.Öncelikle örnek bir DockerFile ele alalım. Burada çok kullanılan bir Floyd Lab tarafından sağlanan hazır bir görüntünün DockerFile dosyasını kullanacağız. Bağlantıdan dosyayı indirebilirsiniz.İndirdiğimiz DockerFile.gpu dosyasının olduğu dizine terminalden gelip aşağıdaki komut ile floydhub/dl-docker adından ve gpu etiketli bir görüntü oluşturuyoruz.$ docker build -t floydhub/dl-docker:gpu -f Dockerfile.gpu .Daha sonra yukarıda yaptığımız gibi bu görüntüyü bir konteyner içinde çalıştırabiliriz.$ docker run -it --gpus all -p 8888:8888 floydhub/dl-docker:gpuŞimdi de DockerFile dosyasının içine bakarak neler yaptığını anlamaya ve sonra özelleştirmek için neler yapabileceğimeze bakalım. (Not: Dosya çok uzun olduğundan bazı bölümleri “…” ile belirterek kısalttığımı belirtmek isterim.)  Öncelikle NVIDIA tarafından sağlanan CUDA’nın 8. sürümü ve cuDNN kütüphanesinin 5. sürümünü kullanan Ubuntu’nun 14.04 sürümünü görüntü içine kuruyor.  Daha sonra kullanacağı bazı parametreleri ARG ile belirliyor.  apt ile gerekli kütüphaneleri kuruyor.  Python paket yöneticisini ve gerekli kütüphaneleri kuruyor.  Tensorflow, Caffe, Theano, Keras, Lasagne, Torch ve Lua derin öğrenme kütüphanelerini kuruyor ve ihtiyaç duyduğu bazı ortam değişkenlerini ayarlıyor.  Açık kaynak bilgisayarlı görü kütüphanelerinden OpenCV kurulumunu yapıyor.  Jupyter Notebook için ayar dosyasını ve Jupyter Notebook kullanımı root kullanıcı için sorunlu olduğundan küçük bir betik dosyası olan  dosyasını kopyalıyor. (Not: Kurulumdan önce floydhub/dl-docker deposundan jupyter_notebook_config.py dosyasını ve birazdan kullanacağımız run_jupyter.sh dosyasını da DockerFile.gpu ile aynı dizine koymamız gerekiyor.  Daha sonra ana makine ile konuşmak üzere 6006 ve 8888 nolu portları açıyor. Genelde 6006 nolu port Tensorboard, 8888 nolu port ise Jupyter Notebook tarafından kullanılmaktadır.  Son olarak görüntü çalıştığında terminalden bash ile bizi karşılayacak komutu yazıyor.$ cat DockerFile.gpuFROM nvidia/cuda:8.0-cudnn5-devel-ubuntu14.04 MAINTAINER Sai Soundararaj &lt;saip@outlook.com&gt;ARG THEANO_VERSION=rel-0.8.2ARG TENSORFLOW_VERSION=0.12.1ARG TENSORFLOW_ARCH=gpu......# Install some dependenciesRUN apt-get update &amp;&amp; apt-get install -y \		bc \		build-essential \		cmake \		curl \		g++ \		gfortran \		git \		libffi-dev \		libfreetype6-dev \		libhdf5-dev \		libjpeg-dev \		liblcms2-dev \		libopenblas-dev \		liblapack-dev \		...		python-dev \		...		&amp;&amp; \	apt-get clean &amp;&amp; \	apt-get autoremove &amp;&amp; \	rm -rf /var/lib/apt/lists/* &amp;&amp; \# Link BLAS library to use OpenBLAS using the alternatives mechanism (https://www.scipy.org/scipylib/building/linux.html#debian-ubuntu)	update-alternatives --set libblas.so.3 /usr/lib/openblas-base/libblas.so.3# Install pipRUN curl -O https://bootstrap.pypa.io/get-pip.py &amp;&amp; \	python get-pip.py &amp;&amp; \	rm get-pip.py# Add SNI support to PythonRUN pip --no-cache-dir install \		pyopenssl \		ndg-httpsclient \		pyasn1# Install useful Python packages using apt-get to avoid version incompatibilities with Tensorflow binary# especially numpy, scipy, skimage and sklearn (see https://github.com/tensorflow/tensorflow/issues/2034)RUN apt-get update &amp;&amp; apt-get install -y \		python-numpy \		python-scipy \		python-nose \		...		&amp;&amp; \	apt-get clean &amp;&amp; \	apt-get autoremove &amp;&amp; \	rm -rf /var/lib/apt/lists/*# Install other useful Python packages using pipRUN pip --no-cache-dir install --upgrade ipython &amp;&amp; \	pip --no-cache-dir install \		Cython \		ipykernel \		jupyter \		path.py \		...		&amp;&amp; \	python -m ipykernel.kernelspec# Install TensorFlowRUN pip --no-cache-dir install \	https://storage.googleapis.com/tensorflow/linux/${TENSORFLOW_ARCH}/tensorflow_${TENSORFLOW_ARCH}-${TENSORFLOW_VERSION}-cp27-none-linux_x86_64.whl# Install dependencies for CaffeRUN apt-get update &amp;&amp; apt-get install -y \		libboost-all-dev \		libgflags-dev \		libgoogle-glog-dev \		libhdf5-serial-dev \		...		&amp;&amp; \	apt-get clean &amp;&amp; \	apt-get autoremove &amp;&amp; \	rm -rf /var/lib/apt/lists/*# Install CaffeRUN git clone -b ${CAFFE_VERSION} --depth 1 https://github.com/BVLC/caffe.git /root/caffe &amp;&amp; \	cd /root/caffe &amp;&amp; \	cat python/requirements.txt | xargs -n1 pip install &amp;&amp; \	mkdir build &amp;&amp; cd build &amp;&amp; \	cmake -DUSE_CUDNN=1 -DBLAS=Open .. &amp;&amp; \	make -j\"$(nproc)\" all &amp;&amp; \	make install# Set up Caffe environment variablesENV CAFFE_ROOT=/root/caffeENV PYCAFFE_ROOT=$CAFFE_ROOT/pythonENV PYTHONPATH=$PYCAFFE_ROOT:$PYTHONPATH \	PATH=$CAFFE_ROOT/build/tools:$PYCAFFE_ROOT:$PATHRUN echo \"$CAFFE_ROOT/build/lib\" &gt;&gt; /etc/ld.so.conf.d/caffe.conf &amp;&amp; ldconfig# Install Theano and set up Theano config (.theanorc) for CUDA and OpenBLASRUN pip --no-cache-dir install git+git://github.com/Theano/Theano.git@${THEANO_VERSION} &amp;&amp; \	\	echo \"[global]\ndevice=gpu\nfloatX=float32\noptimizer_including=cudnn\nmode=FAST_RUN \		\n[lib]\ncnmem=0.95 \		\n[nvcc]\nfastmath=True \		\n[blas]\nldflag = -L/usr/lib/openblas-base -lopenblas \		\n[DebugMode]\ncheck_finite=1\" \	&gt; /root/.theanorc# Install KerasRUN pip --no-cache-dir install git+git://github.com/fchollet/keras.git@${KERAS_VERSION}# Install LasagneRUN pip --no-cache-dir install git+git://github.com/Lasagne/Lasagne.git@${LASAGNE_VERSION}# Install TorchRUN git clone https://github.com/torch/distro.git /root/torch --recursive &amp;&amp; \	cd /root/torch &amp;&amp; \	bash install-deps &amp;&amp; \	yes no | ./install.sh# Export the LUA evironment variables manuallyENV LUA_PATH='/root/.luarocks/share/lua/5.1/?.lua;/root/.luarocks/share/lua/5.1/?/init.lua;/root/torch/install/share/lua/5.1/?.lua;/root/torch/install/share/lua/5.1/?/init.lua;./?.lua;/root/torch/install/share/luajit-2.1.0-beta1/?.lua;/usr/local/share/lua/5.1/?.lua;/usr/local/share/lua/5.1/?/init.lua' \	LUA_CPATH='/root/.luarocks/lib/lua/5.1/?.so;/root/torch/install/lib/lua/5.1/?.so;./?.so;/usr/local/lib/lua/5.1/?.so;/usr/local/lib/lua/5.1/loadall.so' \	PATH=/root/torch/install/bin:$PATH \	LD_LIBRARY_PATH=/root/torch/install/lib:$LD_LIBRARY_PATH \	DYLD_LIBRARY_PATH=/root/torch/install/lib:$DYLD_LIBRARY_PATHENV LUA_CPATH='/root/torch/install/lib/?.so;'$LUA_CPATH# Install the latest versions of nn, cutorch, cunn, cuDNN bindings and iTorchRUN luarocks install nn &amp;&amp; \	luarocks install cutorch &amp;&amp; \	luarocks install cunn &amp;&amp; \    luarocks install loadcaffe &amp;&amp; \	\	cd /root &amp;&amp; git clone https://github.com/soumith/cudnn.torch.git &amp;&amp; cd cudnn.torch &amp;&amp; \	git checkout R4 &amp;&amp; \	luarocks make &amp;&amp; \	\	cd /root &amp;&amp; git clone https://github.com/facebook/iTorch.git &amp;&amp; \	cd iTorch &amp;&amp; \	luarocks make# Install OpenCVRUN git clone --depth 1 https://github.com/opencv/opencv.git /root/opencv &amp;&amp; \	cd /root/opencv &amp;&amp; \	mkdir build &amp;&amp; \	cd build &amp;&amp; \	cmake -DWITH_QT=ON -DWITH_OPENGL=ON -DFORCE_VTK=ON -DWITH_TBB=ON -DWITH_GDAL=ON -DWITH_XINE=ON -DBUILD_EXAMPLES=ON .. &amp;&amp; \	make -j\"$(nproc)\"  &amp;&amp; \	make install &amp;&amp; \	ldconfig &amp;&amp; \	echo 'ln /dev/null /dev/raw1394' &gt;&gt; ~/.bashrc# Set up notebook configCOPY jupyter_notebook_config.py /root/.jupyter/# Jupyter has issues with being run directly: https://github.com/ipython/ipython/issues/7062COPY run_jupyter.sh /root/# Expose Ports for TensorBoard (6006), Ipython (8888)EXPOSE 6006 8888WORKDIR \"/root\"CMD [\"/bin/bash\"]Bazılarınız hazırladığı özgün DockerFile dosyasını uzak depoya göndermek (push) isteyebilir. Yine ayrıntılar için Gökhan Şengün bağlantısından Basit Image Hazırlama ve DockerHub’a Push Etme bölümünde ulaşabilirsiniz.5. Komut Satırı Üzerinden Docker ile EtkileşimÖncelikle var olan görüntülerin listesine terminal üzerinden erişelim:#  Sadece docker images da kullanılabilir.$ sudo docker imagesREPOSITORY                     TAG                       IMAGE ID            CREATED             SIZEhello-world                    latest                    e38bc07ac18e        5 weeks ago         1.85kBgcr.io/tensorflow/tensorflow   latest-gpu_changed        f73dd685943c        5 weeks ago         14.8GBgcr.io/tensorflow/tensorflow   1.7.0-rc0-devel-gpu-py3   a48c5d8684b3        2 months ago        3.1GBGörülebileceği gibi benim ana makinem üzerinde 3 adet görüntü var. Dikkat ederseniz aynı isimli ama farklı etikete sahip iki görüntü var. Altta ilk çektiğim (pull) hali üstte ise zaman içinde konteyner da yaptığım değişiklikleri aktardığım (commit) son halini verdiğim yeni etiketli olan bulunuyor.Konteyner’ı çalıştırmak (run)Görüntüyü oluşturduk. Şimdi görüntüyü bir konteyner da çalıştırmaya sıra geldi.$ sudo docker run -it --gpus all -p 8888:8888  -p 6006:6006 gcr.io/tensorflow/tensorflow:latest-gpu_changed jupyter notebook --allow-rootYukarıdaki komut ile önce docker’a run komutunu it parametreleri ile çalıştırmasını söylüyoruz. i etkileşimli modu ile konteyner çalışınca ona komutlar gönderebilmemiz için STDIN (standart girdiyi) açık tutuyor. t ile konteyner için bir pseudo-TTY tahsis ediliyor. p parametresi ile portların ana makine ile konteyner arasında nasıl yönlendirileceğini söylüyoruz. Burada docker konteynerinin 8888 nolu portu ile ana makinenin 8888. portu ve aynı şekilde 6666. portlarını birbirlerine yönlendirdik. Buna jupyter notebook kullanımında ihtiyaç duyacağız. Sonra hangi görüntünün çalıştırılmasını istediğimizi ve konteyner açılınca jupyter notebook açılmasını istediğimizi docker’a söyledikten sonra işimiz bitiyor. Terminal ekranında aşağıdakine benzer bir çıktı görüyor olmalısınız.$ sudo docker run -it --gpus all -p 8888:8888  -p 6006:6006 gcr.io/tensorflow/tensorflow:latest-gpu_changed jupyter notebook --allow-root[I 14:52:56.460 NotebookApp] Serving notebooks from local directory: /notebooks[I 14:52:56.460 NotebookApp] 0 active kernels[I 14:52:56.460 NotebookApp] The Jupyter Notebook is running at:[I 14:52:56.460 NotebookApp] https://[all ip addresses on your system]:8888/[I 14:52:56.460 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).Şimdi gidip Firefox’u açıp adres satırına “localhost:8888” yazıp sayfaya gittiğimizde aşağıdaki sayfa ile karşılaşıyoruz. Bu sayfada docker konteynerimizin /notebooks klasörünün içeriğine ulaşıyoruz. Docker içerisinde Jupyter Notebook kullanımı ve gerekli ayarların yapılmasını başka bir yazıda aktarmayı planlıyorum. Umarım en kısa zamanda onu da yayımlayacağım. Neyse şimdi konumuza devam edelim.Şimdilik yukarıda sayfanın sağ üstünde bulunan Logout tuşuna basıp bağlantımızı kestikten sonra terminal penceresinde Kontrol+C ile Jupyter sunucusunu ve çalışan docker konteynerimizi kapatıyoruz. Bazen konteyner çalıştığında terminal ekranına çıkmak isteyebilirsiniz. Bu durumda aşağıdaki gibi docker çalıştırma komutumuzun sonuna bash eklemek yeterli olacaktır.$ sudo docker run -it --gpus all -p 8888:8888  -p 6006:6006 gcr.io/tensorflow/tensorflow:latest-gpu_changed bashroot@2fc479bed67f:/notebooks# Görüldüğü gibi artık konterner içinde terminal ekranına bağlıyız ve artık özelleştirmek istersek şimdi güç bizim elimize geçti. Kullanıcı adımız root ve konteyner anahtar adımız 2fc479bed67f (siz de bu alan farklı olacaktır ki bu rasgele verilen bir anahtar) ve aynı Jupyter’de olduğu gibi /notebook klasöründe bulunuyoruz. Terminalde işimiz bittiğinde exit komutu ile çıkıyoruz.Konteyner’da değişiklik yapmak ve içe aktarmak (commit)Görüntüyü oluşturduk ve bir konteyner içinde çalıştırmaya başladık. Bu noktaya kadar sahip olduğumuz içe aktarılmayı bekleyen konteynerleri aşağıdaki komut ile listeyelebiliriz. Burada CONTAINER ID ve NAMES docker motoru tarafından verilen rasgele değerlerdir.$ sudo docker ps -a CONTAINER ID        IMAGE                                             COMMAND             CREATED             STATUS                        PORTS               NAMES2fc479bed67f        gcr.io/tensorflow/tensorflow:latest-gpu_changed   \"bash\"              5 minutes ago       Exited (130) 10 seconds ago                       musing_sahaEğer konteyner içerisinde oluşturulduktan sonra değişiklik yapılmışsa ve bu değişikliği görüntüye aktarmaz isek görüntü her çalıştırıldığında yeni bir konteyner oluşturacağından ve çalışan konteyner diğer konteynerde yapılan değişiklikten haberdar olmadığından bu değişiklikleri daimi olarak kullanmak istememiz halinde içe aktarmamız gerekmektedir. Bunun için aşağıdaki komutu çalıştırmamız yeterli olacaktır.$ sudo docker commit 2fc479bed67f gcr.io/tensorflow/tensorflow:v1sha256:0ddddaf2218987e2ed9f5cfa1976b635a7b811d68d986fef193af6e4c7cfcc30Bu komut ile tag olarak v1 diye bir etiket tanımladığımız başlangıç görüntüsü üzerinde değişikliklerin eklenmiş olduğu yeni bir görüntüye sahip oluyoruz. İstersek etiketi olduğu gibi kullanıp iki ayrı görüntü yerine başlangıç görüntüsü üzerine de aktarım yapabilirdik. Bu noktada sürekli yeni etiketler vererek yola devam etmek bilgisayarınızda daha fazla depolama alanı gerektirecektir.Kullanılmayan Konteynerları durdurmak ve silmek (commit)Çalışan konteynerlerden işimize yaramayanları veya içe aktarmayı tamamladığınız konteynerleri CONTAINER ID parametresini kullanarak durdurmak ve silmek mümkündür. İlk olarak çalışan konteynerleri yukarıda gösterdiğimiz gibi listeleyelim.$ sudo docker ps -aCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                     PORTS                                            NAMES3a9777814a95        ndeep/dl-docker:cpu   \"bash\"                   2 days ago          Exited (255) 2 days ago    0.0.0.0:6006-&gt;6006/tcp, 0.0.0.0:8888-&gt;8888/tcp   festive_chatterjee1a9add8b9905        ndeep/dl-docker:cpu   \"bash\"                   6 weeks ago         Exited (255) 6 weeks ago   0.0.0.0:6006-&gt;6006/tcp, 0.0.0.0:8888-&gt;8888/tcp   dazzling_ridee1a552ab67bf        635015520b19          \"bash\"                   6 weeks ago         Exited (0) 6 weeks ago                                                      objective_northcutt7ea8bd5094c6        635015520b19          \"jupyter notebook --…\"   2 months ago        Exited (0) 2 months ago                                                     priceless_khoranaBu noktada 3a9777814a95 anahtar alanına sahip konteyneri durdurmak için:$ sudo docker stop 3a9777814a953a9777814a95komutunu kullanıyoruz. Durdurduğumuz konteyneri silmek için$ sudo docker rm 3a9777814a953a9777814a95komutunu komut satırına yazmamız gerekiyor. Artık çalışan konteynerleri sıraladığımızda 3a9777814a95 anahtar alanına sahip konteynerdan kurtulmuş oluyoruz.$ sudo docker ps -aCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                     PORTS                                            NAMES1a9add8b9905        ndeep/dl-docker:cpu   \"bash\"                   6 weeks ago         Exited (255) 6 weeks ago   0.0.0.0:6006-&gt;6006/tcp, 0.0.0.0:8888-&gt;8888/tcp   dazzling_ridee1a552ab67bf        635015520b19          \"bash\"                   6 weeks ago         Exited (0) 6 weeks ago                                                      objective_northcutt7ea8bd5094c6        635015520b19          \"jupyter notebook --…\"   2 months ago        Exited (0) 2 months ago                                                     priceless_khoranaGörüntüleri silmekSon olarak; artık işimize yaramayacağını düşündüğümüz görüntüleri silme işlemine bakacağız. Bu noktada bir görüntüyü silmek için önce bu görüntünün çalışan tüm konteynerlerinin durdurulması ve silinmesi gerektiğini hatırlattıktan sonra görüntüyü silme işlemine geçelim. Öncelikle görüntüleri listeyelim:$ sudo docker imagesREPOSITORY           TAG                 IMAGE ID            CREATED             SIZEndeep/dl-docker      cpu2                8403972b7f68        14 minutes ago      11.6GBndeep/dl-docker      cpu1                0ddddaf22189        22 hours ago        11.6GBndeep/dl-docker      cpu                 e16010eb9c55        6 weeks ago         11.6GBfloydhub/dl-docker   cpu_changed         4a4e5cbd6476        5 months ago        11.8GBubuntu               14.04               67759a80360c        6 months ago        221MBBen 8403972b7f68 anahtar alanına sahip görüntüyü silmek istiyorum. Bunun için komut satırına:$ docker rmi 8403972b7f68Untagged: ndeep/dl-docker:cpu2Deleted: sha256:8403972b7f68905eb2bb59efcbee05cefdd8ece92ab28a13d3326d07329437a8komutunu yazdıktan sonra görüntümüzü silebiliyoruz.Sonuç olarak; docker/nvidia-docker derin öğrenme alanında geliştirme/araştırma yapanların nasıl bu aracı kullanabileceğine dair temel bilgileri aktarmaya çalıştım. Elbette docker/nvidia-docker kendilerine has birçok farklı özelliğe sahipler. Ama umarım kısa sürede çalışan bir geliştirme ortamı oluşturabileceksiniz. Jupyter kurulumuna dair konuları başka bir yazıda aktarmaya çalışacağım. (Umarım en kısa zamanda) Eğer sorunuz olursa lütfen aşağıdaki bölümden bana yazınız.            Minimalist OS                C++ Derin Öğrenme-3 Pytorch C++ API LibTorch Modelleri Çalıştırma                C++ Derin Öğrenme-2 Pytorch C++ API LibTorch Tensör İşlemleri                C++ Derin Öğrenme-1 Pytorch C++ API LibTorch Giriş                NVIDIA Docker Kurulumu ve Derin Öğrenme için Kullanımı                NVIDIA Docker Üzerinde Jupyter Ayarları      "
                        } ,
                     
                        {
                          "title"    : "NVIDIA Docker Üzerinde Jupyter Ayarları",
                          "category" : "",
                          "tags"     : " NVIDIA-Docker, Docker, Deep Learning, Derin Öğrenme, Tensorflow, Jupyter, Jupyter Config, Jupyter Notebook",
                          "url"      : "/2018/07/09/docker-jupyter-config.html",
                          "date"     : "July 9, 2018",
                          "excerpt"  : "Python derin/makine öğrenmesi alanında geliştirme yapanların %57‘si tarafından tercih edilen bir programlama dili olarak karşımıza çıkmaktadır.  Buna elbette üssel artan yeni paketler, öğrenme kolaylığı vb. etkenler katkı sağlamaktadır. Programlam...",
                          "content"  : "Python derin/makine öğrenmesi alanında geliştirme yapanların %57‘si tarafından tercih edilen bir programlama dili olarak karşımıza çıkmaktadır.  Buna elbette üssel artan yeni paketler, öğrenme kolaylığı vb. etkenler katkı sağlamaktadır. Programlama dili seçiminden sonra geliştirme ortamının oluşturulması ve geliştirme arayüzünün (IDE-Integrated Developmnent Envirment) kurulması gerekmetedir. Geliştirme ortamının oluşturulması hakkında NVIDIA-Docker Kullanımı isimli yazım da ayrıntılı bilgiye ulaşabilirsiniz. Özellikle de akademisyenler ve öğrenciler özellikle de sunum imkanlarını da değerlendirilerek geliştirme arayüzü olarak Jupyter tercih edilmektedir. NVIDIA-Docker Kullanımı isimli yazımda da görebileceğiniz gibi Docker konteynerini çalıştırdığımızda bizi tarayıcı üzerinden Jupyter Notebook sayfasına ulaşabiliyoruz. Ben de kişisel tercih olarak hem geliştirme hem de sunum maksatlı olarak Jupyter Notebook ile çalışıyorum ve oldukça da keyif alıyorum. Bu yazıda Docker/NVIDIA-Docker üzerinde koşacak Jupyter Notebook ile ilgili ayarlama (config) işlemlerini aktarmaya çalışacağım.Ana Başlıklar  Ayar Dosyasının Oluşturulması  Şifre Oluşturulması  Güvenlik, SSL Bağlantısı Oluşturulması1. Ayar Dosyasının OluşturulmasıTerminal ekranından Docker görüntüsünü bash açılacak şekilde çalıştırmak için aşağıdaki komutu kullanıyoruz:$ sudo nvidia-docker run -it  -p 8888:8888  -p 6006:6006 gcr.io/tensorflow/tensorflow:latest-gpu_changed bashroot@2fc479bed67f:/notebooks#Konteyner içinde notebooks dizinin içindeyiz. Jupyter her çalıştırıldığında tüm ayarları üzerinde barındıran ~/.jupyter dizinin altında bulunan  jupyter_notebook_config.py isimli bir dosyayı okur. Bu dosya mevcut değil ise ilk olarak terminal ekranından:$ jupyter notebook --generate-configkomutu ile ayar dosyasını kolaylıkla oluşturabiliriz.2. Şifre OluşturulmasıJupyter Notebook‘a erişimi şifrelemek doğru bir tercih olacaktır. Bu işlem oldukça basittir. sha1 (Secure Hash Algorithm 1) ile şifrelenmiş bir şifrenin doğrulama kodunu  jupyter_notebook_config.py dosyasının içeriğine eklememiz yeterli olacaktır. Bunun için ilk önce terminal ekranından ipython (etkileşimli olarak Python kodları yazıp çalıştırabildiğimiz bir program) çalıştırılır:$ ipythonDaha sonra bize sha1 ile şifrelenmiş şifremizi oluşturalacak modülü import edip passwd() metodunu çağıracağız. Dilediğimiz şifreyi girip hücreyi çalıştırdığımızda çıktı olarak sha1 ile şifrelenmiş doğrulama kodunu elde etmiş olacağız. Doğrulama kodunu kopyalayıp ipython‘dan exit metodu ile çıkabiliriz.In [1]: from IPython.lib import passwdIn [2]: passwd()Enter password: Verify password:  Out[2]: 'sha1:004ca25bc95e:13156f91c50e71b8e5fd6f7f3a92527dc735bdc2'In [3]: exitvi ile  jupyter_notebook_config.py dosyasını düzenlemek için açıyoruz:$ vi ~/.jupyter/jupyter_notebook_config.pyVe aşağıdaki kodları bu dosyaya ekleyip kaydedip kapatıyoruz. Artık Jupyter Notebook her açılışta bir şifre ekranı bizi karşılayacak ve ipython üzerinden girdiğimiz şifre ile ürettiğimiz doğrulama kodunu kullanarak şifrenin doğru olması halinde Jupyter Notebook ulaşmak mümkün olacak.c = get_config()  # Eğer daha önceden yoksa bu satır eklenecek.c.NotebookApp.password = 'sha1:fc216:3a35a98ed980b9...'  #Doğrulama kodunu buraya ekleyeceğiz. Bundan sonra Jupyter Notebook her açıldığında bizi aşağıdaki gibi bir ekran karşılayacak.3. Güvenlik, SSL Bağlantısı OluşturulmasıBazen Jupyter Notebook ile farklı bilgisayarlardan erişerek çalışmamız gerekebilir. Ben şahsen evdeki bilgisayarımda çalışan NVIDIA-Docker konteynerine uzaktan bağlanmak suretiyle çalışma veya sunum esnasından ulaşarak  Jupyter Notebook kullanıyorum. Şu ana kadar yaptığımız ayarlar şifre hariç bağlantının güvenliğine dair bir tedbir barındırmıyor. Bağlantı güvenliği sağlanmaz ise uzak bilgisardan yerel bilgisayara öntanımlı olarak HTTP üzerinden konuşmak mümkün olabilir. Bu noktada SSL ile şifrelenmiş bir bağlantı kullanarak HTTPS üzerinden konuşmak tercih edilmesi tavisye edilmektedir. Son bölümde güvenli bağlantı için gerekli ayarları uygulayacağız.Öncelikle bağlantının güvenli olması (uçtan uca şifrelenecek olan paketler) için ssl sertifikası üretmemiz gerekiyor. Bunun için:$ cd$ mkdir ssl$ cd ssl$ sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout \"cert.key\" -out \"cert.pem\" -batchkomutlarını kullanarak sırasıyla ev dizinine gidip orada ssl isimli bir dizin oluşturuyoruz. Bu dizinin içerisinde iken openssl ile 365 gün süreli rsa:1024 ile şifrelenmiş bağlantımız için gerekli olan iki dosyayı (sertifika anahtarı ve sertifika) oluştuyoruz. Şimdi yine  jupyter_notebook_config.py ayar dosyamızı düzenleyip aşağıdaki hale dönüştürüyoruz.c = get_config()  # Eğer daha önceden yoksa bu satır eklenecek.c.NotebookApp.certfile = u'~/ssl/cert.pem' # sertifika dosyasının yoluc.NotebookApp.keyfile = u'~/ssl/cert.key' # sertifika anahtar dosyasının yoluc.NotebookApp.password = 'sha1:fc216:3a35a98ed980b9...'  #Doğrulama kodunu buraya ekleyeceğiz. Artık geliştirmeye başlayabiliriz.  İlk defa Jupyter sunucusuna bağlandığınızda aşağıdaki ekran ile karşılaşabilirsiniz. Bu sizin ürettiğiniz sertikanın tanınmamasından kaynaklanıyor. Eğer tarayıcınızdan istisna eklerseniz bir daha sizi bu ekran karşılamayacaktır.    Bu arada tüm bu yaptıklarınızı çalışan konteynerdan çıktıktan sonra her seferinde çalışabilmesi için içe aktarmayı (commit) unutmayın. Aksi halde Docker görüntüsünü çalıştırdığınız zaman içe aktarılmamış değişikliklerin konteyner de geçerli olmayacaktır.NOT: Jupyter sunucusunun çalıştığı ağa bağlanmak için bilgisayarınızda/modeminizde/yönlendiricinizde sabit IP, port yönlendirme vb. bir takım ayarlar yapmanız gerekebilir. Bunun için işletim sistemi/modem marka ve modeline uygun ayarlar için ilgili yönergeleri takip edebilirsiniz. Yine de bu konuda dahil bir sorunuz olursa bana ulaşabilirsiniz."
                        } 
                     ,
                     
                    
                  ],
            searchResultTemplate: '<div class="search-title"><a href="{url}"><h3> {title}</h3></a><div class="meta">{date} <div class="right"><i class="fa fa-tag"></i> {tags}</div></div><p>{excerpt}</p></div><hr> ',
            noResultsText: 'No results found',
            limit: 10,
            fuzzy: false,
            exclude: []
        })
    </script>
</section>
</section>
    
    
  <!-- Tag list for portfolio -->
  
  


<footer>
  <div class="tag-list"></div>
</footer>

    
  <!-- Structured Data for Page -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "WebPage",
    "name": "Search",
    "url": "https://blgnksy.github.io/search/",
    "description": "",
    "inLanguage": "tr",
    "mainEntity": {
      "@type": "Article",
      "headline": "Search",
      "author": {
        "@type": "Person",
        "name": ""
      }
    }
  }
  </script>
    
</article>

    </div>
    
<footer class="site-footer">
    <p class="text">Powered by <a href="https://jekyllrb.com/">Jekyll</a> with <a href="https://github.com/sylhare/Type-on-Strap">Type on Strap</a>
</p>
            <div class="footer-icons">
                <ul>
                <!-- Social icons from Font Awesome, if enabled -->
                


<li>
	<a href="mailto:bilgin.aksoy@metu.edu.tr" title="Email">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>





<li>
	<a href="https://bitbucket.org/blgnksy" title="Follow on Bitbucket">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-bitbucket fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>









<li>
	<a href="https://github.com/blgnksy" title="Follow on GitHub">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>









<li>
	<a href="https://www.linkedin.com/in/bilgin-aksoy-a61a90110" title="Follow on LinkedIn">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>

















<li>
	<a href="https://twitter.com/blgnksy" title="Follow on Twitter" class="type">
        <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>








                </ul>
            </div>
</footer>




  </body>
</html>
<script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'https-blgnksy-github-io'; // required: replace example with your forum shortname
      //var disqus_developer = 1; // Comment out when the site is live

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
    </script>
